# ğŸ“ Natural Language Processing Specialization

### Coursera | DeepLearning.AI | 4-Course Series on NLP ğŸš€

This specialization dives deep into **Natural Language Processing (NLP)**â€”the convergence of linguistics, computer science, and AIâ€”to transform raw text into actionable insights using **state-of-the-art algorithms**.

---

## ğŸŒŸ Key Takeaways:

- Master **sentiment analysis** and **question-answering** models.
- Build powerful tools for **language translation**, **text summarization**, and **chatbots**.
- Gain expertise in **machine learning** and **deep learning** techniques for NLP.

---

## ğŸ—ï¸ Applied Projects Overview:

1. **Logistic Regression** & **NaÃ¯ve Bayes**  
   _Sentiment analysis, analogy completion, language translation, and nearest neighbor search._
2. **Hidden Markov Models** & **Embeddings**  
   _Autocorrect, autocomplete, and part-of-speech (POS) tagging._
3. **RNNs**, **LSTMs**, **GRUs** & **Siamese Networks**  
   _Advanced text generation and duplicate question detection._
4. **Attention Mechanisms**:  
   _Create chatbots, perform question-answering, and explore BERT & Transformer models with ğŸ¤—._

---

## ğŸ§‘â€ğŸ« Taught by Industry Experts:

- **Younes Bensouda Mourri**  
  _Stanford AI Instructor & Co-creator of the Deep Learning Specialization._
- **Åukasz Kaiser**  
  _Google Brain Researcher, Co-author of Tensorflow, Tensor2Tensor, Trax, and the Transformer._

---

# ğŸ“’ Course 1: Natural Language Processing with Classification and Vector Spaces

![](Course_01_Certificate.jpg)

### ğŸ› ï¸ What I Worked On:

1. **Sentiment Analysis with Logistic Regression & NaÃ¯ve Bayes**  
   _Classified tweets to determine positive or negative sentiment using two classic ML techniques._

2. **Vector Space Models & PCA for Word Relationships**  
   _Explored relationships between words using word embeddings and applied **PCA** to reduce dimensionality and visualize them._

3. **English-to-French Translation with Word Embeddings**  
   _Built a simple translation algorithm using pre-computed word embeddings and **locality-sensitive hashing** (LSH) for approximate k-nearest neighbor search._

---

# ğŸ“’ Course 2: Natural Language Processing with Probabilistic Models

### ğŸ› ï¸ What I Worked On:

1. **Auto-Correct Algorithm with Minimum Edit Distance & Dynamic Programming**  
   _Developed a basic spell checker using **minimum edit distance** techniques combined with **dynamic programming** for optimization._

2. **Part-of-Speech (POS) Tagging with the Viterbi Algorithm**  
   _Implemented **Viterbi Algorithm** to tag words with their parts of speechâ€”an essential task in **computational linguistics**._

3. **Auto-Complete Algorithm Using N-gram Language Model**  
   _Created a smarter **auto-complete** system leveraging an **N-gram language model** to predict the next word in a sequence._

4. **Custom Word2Vec Model with Neural Networks**  
   _Built my own **Word2Vec** implementation using a **continuous bag-of-words (CBOW)** neural network to compute word embeddings._

---

---

---
